{
  "questions": [
    {"id": "1",  "namespace": "ckad-q01", "machineHostname": "ckad9999", "question": "Discover all Kubernetes namespaces and persist the exact output to a file for later review.\n\n**Instructions:**\n1. Use `kubectl` to list all namespaces in the cluster in a tabular form that includes the `NAME` header.\n2. Save the output to `/opt/course/exam3/q01/namespaces`.\n3. Do not alter the output; the file must contain the word `default`.\n\n**Hints:**\n* A common command is `kubectl get namespaces` (alias: `kubectl get ns`).\n* Ensure the output is redirected to the file, for example: `kubectl get ns > /opt/course/exam3/q01/namespaces`.\n\n**Verification:**\n* The file exists at `/opt/course/exam3/q01/namespaces` and contains the `NAME` header and the `default` namespace.", "concepts": ["namespaces"], "verification": [
      {"id": "1", "description": "Q1: namespaces file exists", "verificationScriptFile": "q1_s1_validate_file_exists.sh", "expectedOutput": "0", "weightage": 1},
      {"id": "2", "description": "Q1: file contains default namespace", "verificationScriptFile": "q1_s3_validate_contains_default.sh", "expectedOutput": "0", "weightage": 1}
    ]},
    {"id": "2",  "namespace": "ckad-q02", "machineHostname": "ckad9999", "question": "Your task is to create a new Pod and a status script in the dedicated namespace for this question.\n\n**Instructions:**\n1.  **Create a Pod:**\n    *   Name the Pod: `pod1`\n    *   Use the image: `httpd:2.4.41-alpine`\n    *   Set the container name within the Pod to: `pod1-container`\n    *   Ensure this Pod is created in the `ckad-q02` namespace.\n2.  **Create a Status Script:**\n    *   Develop a command that outputs the current status (`.status.phase`) of the `pod1` Pod.\n    *   Save this command as an executable script file: `/opt/course/exam3/q02/pod1-status-command.sh`\n\n**Verification:**\n*   The Pod `pod1` must be running in the `ckad-q02` namespace.\n*   The `pod1-container` container must exist within the `pod1` Pod.\n*   The script file `/opt/course/exam3/q02/pod1-status-command.sh` must exist and be executable.", "concepts": ["pods"], "verification": [
      {"id": "1", "description": "Q2: pod pod1 exists", "verificationScriptFile": "q2_s1_validate_pod_exists.sh", "expectedOutput": "0", "weightage": 1},
      {"id": "2", "description": "Q2: container name is pod1-container", "verificationScriptFile": "q2_s2_validate_container_name.sh", "expectedOutput": "0", "weightage": 1},
      {"id": "3", "description": "Q2: status script exists and is executable", "verificationScriptFile": "q2_s3_validate_status_script.sh", "expectedOutput": "0", "weightage": 2}
    ]},
    {"id": "3",  "namespace": "ckad-q03", "machineHostname": "ckad9999", "question": "Team Neptune needs a new Job. Your task is to create this Job according to the specifications below and save its configuration.\n\n**Instructions:**\n1.  **Create a Job:**\n    *   Name the Job: `neb-new-job`\n    *   Use the image: `busybox:1.31.0`\n    *   The command for the container should be: `sleep 2 && echo done`\n    *   Set `completions` to `3` (the Job should run to completion 3 times).\n    *   Set `parallelism` to `2` (a maximum of 2 Pods should run at any given time).\n    *   The container within the Job's Pods should be named: `neb-new-job-container`\n    *   Each Pod created by the Job should have the label: `id=awesome-job`\n    *   Ensure this Job is created in the `ckad-q03` namespace.\n2.  **Save Configuration:**\n    *   Save the YAML definition of the Job to the file: `/opt/course/exam3/q03/job.yaml`\n3.  **Deploy and Verify:**\n    *   Create the Job using your saved YAML file.\n    *   Verify that the Job eventually completes all 3 runs with a maximum of 2 parallel executions.\n\n**Verification:**\n*   The Job `neb-new-job` must exist and be running in the `ckad-q03` namespace.\n*   The Job's `completions` parameter must be set to `3`.\n*   The Job's `parallelism` parameter must be set to `2`.\n*   The Pods created by the Job must have the label `id=awesome-job`.\n*   The YAML definition of the Job must be saved to `/opt/course/exam3/q03/job.yaml`.", "concepts": ["jobs"], "verification": [
      {"id": "1", "description": "Q3: job exists", "verificationScriptFile": "q3_s1_validate_job_exists.sh", "expectedOutput": "0", "weightage": 1},
      {"id": "2", "description": "Q3: completions=3 and parallelism=2", "verificationScriptFile": "q3_s2_validate_job_spec.sh", "expectedOutput": "0", "weightage": 2},
      {"id": "3", "description": "Q3: job yaml file exists", "verificationScriptFile": "q3_s3_validate_file_exists.sh", "expectedOutput": "0", "weightage": 1}
    ]},
    {"id": "4",  "namespace": "ckad-q04", "machineHostname": "ckad9999", "question": "Team Mercury has asked you to perform several Helm operations within the `ckad-q04` namespace.\n\n**Pre-requisites:**\n*   Ensure the Helm repository `killershell` is added and updated:\n    ```bash\n    helm repo add killershell http://localhost:6000\n    helm repo update\n    ```\n\n**Instructions:**\n1.  **Delete a Release:** Delete the Helm release named `internal-issue-report-apiv1`.\n2.  **Upgrade a Release:** Upgrade the Helm release named `internal-issue-report-apiv2` to any newer version of the `killershell/nginx` chart.\n3.  **Install a New Release:** Install a new Helm release named `internal-issue-report-apache` using the `killershell/apache` chart. This Deployment should have `2 replicas`. Ensure you set the replica count via Helm values during installation.\n4.  **Clean Up Pending Releases:** Identify and delete any Helm releases that are stuck in a `pending-install` state.\n\n**Verification:**\n*   The `internal-issue-report-apiv1` release should no longer exist in the `ckad-q04` namespace.\n*   The `internal-issue-report-apiv2` release should be upgraded to a newer chart version.\n*   The `internal-issue-report-apache` release should be successfully installed with `2 replicas`.\n*   Any releases in a `pending-install` state should be removed.", "concepts": ["helm"], "verification": [
      {"id": "1", "description": "Q4: apiv1 not present", "verificationScriptFile": "q4_s1_validate_apiv1_absent.sh", "expectedOutput": "0", "weightage": 1},
      {"id": "2", "description": "Q4: apiv2 present", "verificationScriptFile": "q4_s2_validate_apiv2_present.sh", "expectedOutput": "0", "weightage": 1},
      {"id": "3", "description": "Q4: apache replicas=2", "verificationScriptFile": "q4_s3_validate_apache_replicas.sh", "expectedOutput": "0", "weightage": 1}
    ]},
    {"id": "5",  "namespace": "ckad-q05", "machineHostname": "ckad9999", "question": "Team Neptune requires assistance with a ServiceAccount and its associated Secret token within the `ckad-q05` namespace.\n\n**Instructions:**\n1.  **Locate ServiceAccount Token:**\n    *   An existing ServiceAccount named `neptune-sa-v2` is present in the `ckad-q05` namespace.\n    *   Identify the Secret that is automatically generated and associated with this ServiceAccount. This Secret contains the ServiceAccount's token.\n2.  **Extract and Decode Token:**\n    *   Retrieve the token from the identified Secret.\n    *   The token will be Base64 encoded. Decode this token.\n3.  **Save Decoded Token:**\n    *   Write the Base64 decoded token to the file: `/opt/course/exam3/q05/token` on the `localhost` instance.\n\n**Verification:**\n*   A file named `/opt/course/exam3/q05/token` must exist.\n*   The file `/opt/course/exam3/q05/token` must contain the Base64 decoded token and not be empty.", "concepts": ["serviceaccounts","secrets"], "verification": [
      {"id": "1", "description": "Q5: ServiceAccount exists", "verificationScriptFile": "q5_s1_validate_sa_exists.sh", "expectedOutput": "0", "weightage": 1},
      {"id": "2", "description": "Q5: token file exists and non-empty", "verificationScriptFile": "q5_s2_validate_token_file.sh", "expectedOutput": "0", "weightage": 2}
    ]},
    {"id": "6",  "namespace": "ckad-q06", "machineHostname": "ckad9999", "question": "Create a Pod that only becomes Ready after a marker file is present.\n\n**Instructions:**\n1. Create a Pod named `pod6` in the `ckad-q06` namespace.\n2. The container image should be `busybox:1.31.0`.\n3. Configure a readinessProbe that executes `cat /tmp/ready`.\n4. Ensure the container creates the marker file so that the probe eventually passes. A simple approach is a command like: `sh -c 'sleep 2; echo ok > /tmp/ready; sleep 1d'`.\n\n**Outcome:**\n* The Pod `pod6` exists in `ckad-q06` and its Ready condition becomes True.\n\n**Hints:**\n* Use the `exec` type readiness probe with `command: [\"cat\", \"/tmp/ready\"]`.\n* You can specify the container `command`/`args` to create the file before sleeping.\n", "concepts": ["probes","pods"], "verification": [
      {"id": "1", "description": "Q6: pod6 exists", "verificationScriptFile": "q6_s1_validate_pod_exists.sh", "expectedOutput": "0", "weightage": 1},
      {"id": "2", "description": "Q6: pod6 is Ready", "verificationScriptFile": "q6_s2_validate_pod_ready.sh", "expectedOutput": "0", "weightage": 2}
    ]},
    {"id": "7",  "namespace": "ckad-q07", "machineHostname": "ckad9999", "question": "A running Pod needs to be relocated between namespaces without changing its behavior. Perform the move by recreating the Pod in the target namespace and removing it from the source.\n\n**Scope:**\nAll actions must be performed on the single cluster, using the dedicated namespaces for this task.\n\n**Source State (already seeded):**\n* Namespace: `ckad-q07-source`\n* Pod Name: `webserver-sat-003`\n* Container Name: `webserver-sat`\n* Image: `nginx:1.16.1-alpine`\n* Labels: `id=webserver-sat-003`\n* Annotation: `description=\"this is the server for the E-Commerce System my-happy-shop\"`\n\n**Target State (what you must achieve):**\n* Namespace: `ckad-q07-target`\n* A Pod named `webserver-sat-003` exists and is Running.\n* The Pod preserves the same container name, image, labels, and annotations as the source Pod.\n* The Pod no longer exists in `ckad-q07-source`.\n\n**Task:**\n1. Re-create the Pod `webserver-sat-003` in namespace `ckad-q07-target` with the exact same spec (container name, image, labels, and annotations).\n2. Remove the original Pod from `ckad-q07-source`.\n3. Save the final manifest you used for the target Pod to `/opt/course/exam3/q07/webserver-sat-003.yaml`.\n\n**Notes and Hints:**\n* You cannot literally \"move\" a Pod between namespaces; re-create it in the new namespace.\n* A practical approach is to export the source Pod spec, adjust only the `metadata.namespace`, and apply it in the target: for example, `kubectl -n ckad-q07-source get pod webserver-sat-003 -o yaml > /opt/course/exam3/q07/webserver-sat-003.yaml`, then edit the file and `kubectl -n ckad-q07-target apply -f /opt/course/exam3/q07/webserver-sat-003.yaml`. Finally, delete the source Pod.\n\n**Verification:**\n* The Pod `webserver-sat-003` is absent from `ckad-q07-source`.\n* The Pod `webserver-sat-003` exists in `ckad-q07-target`.\n", "concepts": ["pods","namespaces"], "verification": [
      {"id": "1", "description": "Q7: pod not in source", "verificationScriptFile": "q7_s1_validate_absent_in_source.sh", "expectedOutput": "0", "weightage": 1},
      {"id": "2", "description": "Q7: pod in target", "verificationScriptFile": "q7_s2_validate_present_in_target.sh", "expectedOutput": "0", "weightage": 2}
    ]},
    {"id": "8",  "namespace": "ckad-q08", "machineHostname": "ckad9999", "question": "A Deployment rollout introduced a regression and must be rolled back.\n\n**Current State:**\n* Namespace: `ckad-q08`\n* Deployment: `api-new-c32`\n\n**Task:**\n1. Use `kubectl` to roll back the Deployment `api-new-c32` to the last working revision.\n2. Wait until the Deployment is fully Available (all desired replicas are available).\n\n**Hints:**\n* Useful commands: `kubectl rollout history deploy api-new-c32`, `kubectl rollout undo deploy api-new-c32`, and `kubectl rollout status deploy api-new-c32`.\n\n**Verification:**\n* The Deployment `api-new-c32` exists in `ckad-q08` and has all desired replicas available.", "concepts": ["deployments","rollouts"], "verification": [
      {"id": "1", "description": "Q8: deployment exists", "verificationScriptFile": "q8_s1_validate_deploy_exists.sh", "expectedOutput": "0", "weightage": 1},
      {"id": "2", "description": "Q8: deployment ready", "verificationScriptFile": "q8_s2_validate_deploy_ready.sh", "expectedOutput": "0", "weightage": 3}
    ]},
    {"id": "9",  "namespace": "ckad-q09", "machineHostname": "ckad9999", "question": "Migrate a single Pod to a Deployment for resilience and scaling.\n\n**Current State:**\n* Namespace: `ckad-q09`\n* Pod: `holy-api` (assume its image/spec should be preserved)\n\n**Task:**\n1. Create a Deployment named `holy-api` with `3` replicas in `ckad-q09` based on the existing Pod's spec.\n2. Save the Deployment manifest you applied to `/opt/course/exam3/q09/holy-api-deployment.yaml`.\n\n**Hints:**\n* You can generate a starting point with `kubectl create deploy holy-api --image=<image> -n ckad-q09 -o yaml --dry-run=client > /opt/course/exam3/q09/holy-api-deployment.yaml` and then adjust fields (labels, ports, etc.).\n* Alternatively, capture the existing Pod spec and map it into a Deployment template.\n\n**Verification:**\n* A Deployment `holy-api` exists in `ckad-q09` with `.spec.replicas=3`.\n* The file `/opt/course/exam3/q09/holy-api-deployment.yaml` exists.", "concepts": ["deployments","security"], "verification": [
      {"id": "1", "description": "Q9: deployment exists", "verificationScriptFile": "q9_s1_validate_deploy_exists.sh", "expectedOutput": "0", "weightage": 1},
      {"id": "2", "description": "Q9: replicas=3", "verificationScriptFile": "q9_s2_validate_replicas.sh", "expectedOutput": "0", "weightage": 1},
      {"id": "3", "description": "Q9: YAML file exists", "verificationScriptFile": "q9_s3_validate_file_exists.sh", "expectedOutput": "0", "weightage": 1}
    ]},
    {"id": "10", "namespace": "ckad-q10", "machineHostname": "ckad9999", "question": "Expose a Pod with a ClusterIP Service and validate connectivity.\n\n**Current State:**\n* Namespace: `ckad-q10`\n* Pod: `project-plt-6cc-api` (listening on container port `80`)\n\n**Task:**\n1. Create a Service named `project-plt-6cc-svc` of type `ClusterIP` in `ckad-q10`.\n2. Map Service `port: 3333` to `targetPort: 80` of the Pod.\n3. From within the cluster, `curl` the Service on port 3333 and save the response HTML to `/opt/course/exam3/q10/service_test.html`.\n\n**Hints:**\n* `kubectl -n ckad-q10 expose pod project-plt-6cc-api --name project-plt-6cc-svc --port 3333 --target-port 80 --type ClusterIP`.\n* To curl from within the cluster, you can `kubectl -n ckad-q10 run tmp --rm -it --image=busybox:1.31.0 -- wget -qO- project-plt-6cc-svc:3333`.\n\n**Verification:**\n* The Service has `port 3333` and `targetPort 80`.\n* The file `/opt/course/exam3/q10/service_test.html` exists.", "concepts": ["services","pods","logs"], "verification": [
      {"id": "1", "description": "Q10: service 3333â†’80 mapping", "verificationScriptFile": "q10_s1_validate_service_ports.sh", "expectedOutput": "0", "weightage": 1},
      {"id": "2", "description": "Q10: pod exists", "verificationScriptFile": "q10_s2_validate_pod_exists.sh", "expectedOutput": "0", "weightage": 1},
      {"id": "3", "description": "Q10: output file exists", "verificationScriptFile": "q10_s3_validate_output_file.sh", "expectedOutput": "0", "weightage": 1}
    ]},
    {"id": "11", "namespace": "ckad-q11", "machineHostname": "ckad9999", "question": "Build and run a container image that emits a specific environment variable in its logs.\n\n**Goal:**\nEnsure logs contain the string `SUN_CIPHER_ID`. Save the logs to `/opt/course/exam3/q11/logs`.\n\n**Primary Path (if image build/push is available):**\n1. Create a simple container image (using Docker or Podman) that prints the value of an environment variable `SUN_CIPHER_ID` on startup.\n2. Run the image in the `ckad-q11` namespace, setting `SUN_CIPHER_ID` via environment.\n3. Capture logs to `/opt/course/exam3/q11/logs`.\n\n**Fallback Path (if no builder/registry):**\n1. Run a Pod in `ckad-q11` using a base image (e.g., `busybox:1.31.0`) with a command that echoes `SUN_CIPHER_ID=$SUN_CIPHER_ID` and then sleeps.\n2. Set the env var on the container.\n3. Fetch the Pod logs and save to `/opt/course/exam3/q11/logs`.\n\n**Verification:**\n* The logs file exists and contains `SUN_CIPHER_ID`.", "concepts": ["images","containers"], "verification": [
      {"id": "1", "description": "Q11: logs file exists", "verificationScriptFile": "q11_s1_validate_logs_file.sh", "expectedOutput": "0", "weightage": 1},
      {"id": "2", "description": "Q11: logs contain SUN_CIPHER_ID", "verificationScriptFile": "q11_s2_validate_logs_content.sh", "expectedOutput": "0", "weightage": 1}
    ]},
    {"id": "12", "namespace": "ckad-q12", "machineHostname": "ckad9999", "question": "Provide persistent storage to a Deployment via a bound PVC.\n\n**Task:**\n1. Create a PersistentVolume named `earth-project-earthflower-pv` (e.g., `hostPath` suitable for single-node labs) with `capacity: 1Gi`, `accessModes: [ReadWriteOnce]`, and a unique `storageClassName` or no class.\n2. Create a PersistentVolumeClaim named `earth-project-earthflower-pvc` in namespace `ckad-q12` that binds to the PV (matching size and access mode).\n3. Create or patch a Deployment `project-earthflower` in `ckad-q12` that mounts the PVC at `/tmp/project-data`.\n\n**Hints:**\n* Example PV (single-node): `hostPath: /tmp/earthflower-pv-q12`.\n* Ensure the PVC and PV specs align so the PVC becomes Bound.\n\n**Verification:**\n* PV exists cluster-wide.\n* PVC `earth-project-earthflower-pvc` is Bound in `ckad-q12`.\n* Deployment `project-earthflower` exists.", "concepts": ["storage","pv","pvc","deployments"], "verification": [
      {"id": "1", "description": "Q12: PV exists", "verificationScriptFile": "q12_s1_validate_pv_exists.sh", "expectedOutput": "0", "weightage": 1},
      {"id": "2", "description": "Q12: PVC exists", "verificationScriptFile": "q12_s2_validate_pvc_exists.sh", "expectedOutput": "0", "weightage": 1},
      {"id": "3", "description": "Q12: PVC is Bound", "verificationScriptFile": "q12_s3_validate_pvc_bound.sh", "expectedOutput": "0", "weightage": 1},
      {"id": "4", "description": "Q12: deployment exists", "verificationScriptFile": "q12_s4_validate_deploy_exists.sh", "expectedOutput": "0", "weightage": 1}
    ]},
    {"id": "13", "namespace": "ckad-q13", "machineHostname": "ckad9999", "question": "Create a non-provisioning StorageClass and observe a Pending PVC, capturing the reason.\n\n**Task:**\n1. Create a StorageClass named `moon-retain` with `reclaimPolicy: Retain` and a non-existent provisioner (e.g., `example.com/no-provisioner`). Do not set `volumeBindingMode` unless desired.\n2. Create a PVC named `moon-pvc-126` in `ckad-q13` that requests `1Gi` `ReadWriteOnce` and references `storageClassName: moon-retain`.\n3. The PVC should remain in `Pending` phase. Extract the explanation/reason and write it to `/opt/course/exam3/q13/pvc-126-reason`.\n\n**Hints:**\n* Use `kubectl describe pvc moon-pvc-126 -n ckad-q13` and look under `Events` or `Reason`.\n\n**Verification:**\n* StorageClass `moon-retain` exists.\n* PVC `moon-pvc-126` exists and is Pending.\n* Reason file exists and is not empty.", "concepts": ["storageclass","pvc"], "verification": [
      {"id": "1", "description": "Q13: StorageClass exists", "verificationScriptFile": "q13_s1_validate_sc_exists.sh", "expectedOutput": "0", "weightage": 1},
      {"id": "2", "description": "Q13: PVC exists", "verificationScriptFile": "q13_s2_validate_pvc_exists.sh", "expectedOutput": "0", "weightage": 1},
      {"id": "3", "description": "Q13: PVC is Pending", "verificationScriptFile": "q13_s3_validate_pvc_pending.sh", "expectedOutput": "0", "weightage": 1},
      {"id": "4", "description": "Q13: reason file exists", "verificationScriptFile": "q13_s4_validate_reason_file.sh", "expectedOutput": "0", "weightage": 1}
    ]},
    {"id": "14", "namespace": "ckad-q14", "machineHostname": "ckad9999", "question": "Wire a Pod to consume a Secret as env and a ConfigMap as a volume, then save the updated manifest.\n\n**Current State:**\n* Namespace: `ckad-q14`\n* Pod: `secret-handler` exists with an emptyDir volume mounted at `/tmp/secret2`.\n\n**Task:**\n1. Create a Secret named `secret1` in `ckad-q14` with at least one key/value (e.g., `username=moon` or similar).\n2. Create a ConfigMap named `secret2` with some sample data.\n3. Update the Pod `secret-handler` so that:\n   * It has environment variables sourced from Secret `secret1` (e.g., using `envFrom`).\n   * The volume `secret2` is sourced from the ConfigMap `secret2` and remains mounted at `/tmp/secret2`.\n4. Save the updated manifest to `/opt/course/exam3/q14/secret-handler-new.yaml`.\n\n**Verification:**\n* Secret `secret1` exists.\n* Pod `secret-handler` exists.\n* Updated YAML file exists.", "concepts": ["secrets","pods"], "verification": [
      {"id": "1", "description": "Q14: secret1 exists", "verificationScriptFile": "q14_s1_validate_secret_exists.sh", "expectedOutput": "0", "weightage": 1},
      {"id": "2", "description": "Q14: pod secret-handler exists", "verificationScriptFile": "q14_s2_validate_pod_exists.sh", "expectedOutput": "0", "weightage": 1},
      {"id": "3", "description": "Q14: updated YAML exists", "verificationScriptFile": "q14_s3_validate_file_exists.sh", "expectedOutput": "0", "weightage": 1}
    ]},
    {"id": "15", "namespace": "ckad-q15", "machineHostname": "ckad9999", "question": "Provide HTML content to an NGINX Deployment via ConfigMap.\n\n**Current State:**\n* Namespace: `ckad-q15`\n* Deployment `web-moon` mounts `ConfigMap` `configmap-web-moon-html` at `/usr/share/nginx/html`.\n\n**Task:**\n1. Create a ConfigMap named `configmap-web-moon-html` in `ckad-q15` containing a key `index.html` with some HTML content.\n2. Save the ConfigMap manifest to `/opt/course/exam3/q15/configmap.yaml`.\n3. Ensure the Deployment uses this ConfigMap as shown (you do not need to change the Deployment if it already references the correct name).\n\n**Hints:**\n* You can create from a literal file or inline using `--from-literal` or `--from-file=index.html=/path/to/file`.\n\n**Verification:**\n* ConfigMap exists.\n* ConfigMap YAML file exists.", "concepts": ["configmaps","deployments"], "verification": [
      {"id": "1", "description": "Q15: ConfigMap exists", "verificationScriptFile": "q15_s1_validate_configmap_exists.sh", "expectedOutput": "0", "weightage": 2},
      {"id": "2", "description": "Q15: configmap.yaml exists", "verificationScriptFile": "q15_s2_validate_file_exists.sh", "expectedOutput": "0", "weightage": 1}
    ]},
    {"id": "16", "namespace": "ckad-q16", "machineHostname": "ckad9999", "question": "Add a sidecar container to stream application logs, then save the updated manifest.\n\n**Current State:**\n* Namespace: `ckad-q16`\n* Deployment: `cleaner` writes logs to `/var/log/cleaner/cleaner.log`.\n\n**Task:**\n1. Add a sidecar container (e.g., `name: logger`, `image: busybox:1.31.0`) that tails `/var/log/cleaner/cleaner.log` to stdout using `sh -c 'tail -F /var/log/cleaner/cleaner.log'`.\n2. Mount the same volume `logvol` at the same path in the sidecar.\n3. Save the updated manifest to `/opt/course/exam3/q16/cleaner-new.yaml`.\n\n**Verification:**\n* Deployment `cleaner` exists.\n* Updated YAML file exists.", "concepts": ["logging","sidecar"], "verification": [
      {"id": "1", "description": "Q16: deployment exists", "verificationScriptFile": "q16_s1_validate_deploy_exists.sh", "expectedOutput": "0", "weightage": 1},
      {"id": "2", "description": "Q16: cleaner-new.yaml exists", "verificationScriptFile": "q16_s2_validate_file_exists.sh", "expectedOutput": "0", "weightage": 2}
    ]},
    {"id": "17", "namespace": "ckad-q17", "machineHostname": "ckad9999", "question": "Add an initContainer that prepares web content, then save the updated manifest.\n\n**Current State:**\n* Namespace: `ckad-q17`\n* A base Deployment manifest is stored at `/opt/course/exam3/q17/test-init-container.yaml`, with a writable `emptyDir` volume `site` mounted to NGINX.\n\n**Task:**\n1. Add an `initContainer` that writes an `index.html` file into the shared volume `site`, for example: `sh -c 'echo Hello from init > /work/index.html'` with the mount path matching the volume mount.\n2. Save the updated manifest to `/opt/course/exam3/q17/test-init-container-new.yaml`.\n\n**Verification:**\n* Deployment exists.\n* Updated YAML file exists.", "concepts": ["initcontainers","volumes"], "verification": [
      {"id": "1", "description": "Q17: deployment exists", "verificationScriptFile": "q17_s1_validate_deploy_exists.sh", "expectedOutput": "0", "weightage": 1},
      {"id": "2", "description": "Q17: test-init-container-new.yaml exists", "verificationScriptFile": "q17_s2_validate_file_exists.sh", "expectedOutput": "0", "weightage": 2}
    ]},
    {"id": "18", "namespace": "ckad-q18", "machineHostname": "ckad9999", "question": "Fix a misconfigured Service so it selects the correct Pods and exposes the right port.\n\n**Current State:**\n* Namespace: `ckad-q18`\n* Deployment: `manager-api-deployment` with Pods labeled `app=manager-api`, container port `80`.\n* Service: `manager-api-svc` with an incorrect selector and `port: 4444`.\n\n**Task:**\n1. Update the Service selector so it matches the Deployment Pods (label `app=manager-api`).\n2. Keep targetPort at `80`. The `port` may remain as-is or be adjusted as needed; the key requirement is that Endpoints are populated.\n3. Verify that the Service has populated Endpoints for the backing Pods.\n\n**Verification:**\n* Service exists.\n* Endpoints for `manager-api-svc` are non-empty.", "concepts": ["services","selectors"], "verification": [
      {"id": "1", "description": "Q18: service exists", "verificationScriptFile": "q18_s1_validate_service_exists.sh", "expectedOutput": "0", "weightage": 1},
      {"id": "2", "description": "Q18: service has endpoints", "verificationScriptFile": "q18_s2_validate_endpoints.sh", "expectedOutput": "0", "weightage": 2}
    ]},
    {"id": "19", "namespace": "ckad-q19", "machineHostname": "ckad9999", "question": "Expose an existing Service externally via NodePort and set a specific port.\n\n**Current State:**\n* Namespace: `ckad-q19`\n* Deployment: `jupiter-crew-deploy` (httpd) and Service `jupiter-crew-svc` (ClusterIP).\n\n**Task:**\n1. Change the Service `jupiter-crew-svc` type to `NodePort`.\n2. Set `nodePort: 30100`.\n3. Optionally verify reachability inside the cluster using the Service ClusterIP or via NodePort if on a single-node cluster.\n\n**Hints:**\n* You can `kubectl -n ckad-q19 patch svc jupiter-crew-svc -p '{\"spec\":{\"type\":\"NodePort\",\"ports\":[{\"port\":80,\"targetPort\":80,\"nodePort\":30100}]}}'`.\n\n**Verification:**\n* Service type is NodePort.\n* nodePort is `30100`.", "concepts": ["services","nodeport"], "verification": [
      {"id": "1", "description": "Q19: service is NodePort", "verificationScriptFile": "q19_s1_validate_type_nodeport.sh", "expectedOutput": "0", "weightage": 1},
      {"id": "2", "description": "Q19: nodePort=30100", "verificationScriptFile": "q19_s2_validate_nodeport_value.sh", "expectedOutput": "0", "weightage": 2}
    ]},
    {"id": "20", "namespace": "ckad-p1",  "machineHostname": "ckad9999", "question": "Add a TCP liveness probe to an existing Deployment and save the updated manifest.\n\n**Current State:**\n* Namespace: `ckad-p1`\n* Deployment manifest at `/opt/course/exam3/p1/project-23-api.yaml` defines `project-23-api` listening on container port `80`.\n\n**Task:**\n1. Add a `livenessProbe` using `tcpSocket` on port `80` to the container.\n2. Save the updated manifest to `/opt/course/exam3/p1/project-23-api-new.yaml`.\n3. Apply the updated manifest and ensure the Deployment becomes healthy.\n\n**Verification:**\n* Deployment exists.\n* Updated YAML file exists.", "concepts": ["probes","deployments"], "verification": [
      {"id": "1", "description": "P1: deployment exists", "verificationScriptFile": "q20_s1_validate_deploy_exists.sh", "expectedOutput": "0", "weightage": 1},
      {"id": "2", "description": "P1: updated YAML exists", "verificationScriptFile": "q20_s2_validate_file_exists.sh", "expectedOutput": "0", "weightage": 1}
    ]},
    {"id": "21", "namespace": "ckad-p2",  "machineHostname": "ckad9999", "question": "Create and expose a Deployment using a provided ServiceAccount, then add a status script.\n\n**Current State:**\n* Namespace: `ckad-p2`\n* ServiceAccount `sa-sun-deploy` already exists.\n\n**Task:**\n1. Create a Deployment named `sunny` with `4` replicas in `ckad-p2` using image `nginx:1.17.3-alpine` (or similar) and set `serviceAccountName: sa-sun-deploy`.\n2. Expose the Deployment with a Service named `sun-srv` on `port: 9999` targeting container port `80`.\n3. Create an executable script `/opt/course/exam3/p2/sunny_status_command.sh` that prints the Deployment's `.status.readyReplicas` or rollout status.\n\n**Hints:**\n* Example status commands:\n  * `kubectl -n ckad-p2 get deploy sunny -o jsonpath='{.status.readyReplicas}'`\n  * `kubectl -n ckad-p2 rollout status deploy/sunny`\n\n**Verification:**\n* Deployment exists with 4 replicas.\n* Service `sun-srv` exists.\n* Status script exists and is executable.", "concepts": ["deployments","serviceaccounts","services"], "verification": [
      {"id": "1", "description": "P2: deployment exists", "verificationScriptFile": "q21_s1_validate_deploy_exists.sh", "expectedOutput": "0", "weightage": 1},
      {"id": "2", "description": "P2: replicas=4", "verificationScriptFile": "q21_s2_validate_replicas.sh", "expectedOutput": "0", "weightage": 1},
      {"id": "3", "description": "P2: service sun-srv exists", "verificationScriptFile": "q21_s3_validate_service_exists.sh", "expectedOutput": "0", "weightage": 1},
      {"id": "4", "description": "P2: status script exists", "verificationScriptFile": "q21_s4_validate_status_script.sh", "expectedOutput": "0", "weightage": 1}
    ]},
    {"id": "22", "namespace": "ckad-p3",  "machineHostname": "ckad9999", "question": "Repair a failing readiness probe and document the issue.\n\n**Current State:**\n* Namespace: `ckad-p3`\n* Deployment: `earth-3cc-web` with `replicas: 4`.\n* The container's `readinessProbe` uses a `tcpSocket` on port `81` while the container serves port `80`.\n\n**Task:**\n1. Update the readiness probe to check the correct port (`80`).\n2. Apply the change and ensure the Deployment becomes fully Available.\n3. Write a short description of the root cause and fix to `/opt/course/exam3/p3/ticket-description.txt`.\n\n**Verification:**\n* Deployment exists and all replicas are available.\n* Description file exists.", "concepts": ["probes","services"], "verification": [
      {"id": "1", "description": "P3: deployment exists", "verificationScriptFile": "q22_s1_validate_deploy_exists.sh", "expectedOutput": "0", "weightage": 1},
      {"id": "2", "description": "P3: deployment ready (all replicas)", "verificationScriptFile": "q22_s2_validate_deploy_ready.sh", "expectedOutput": "0", "weightage": 1},
      {"id": "3", "description": "P3: ticket-description.txt exists", "verificationScriptFile": "q22_s3_validate_file_exists.sh", "expectedOutput": "0", "weightage": 1}
    ]}
  ]
}
